{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial and Computational Intelligence (Assignment - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "As part of the 2nd Assignment, we'll try and predict the Part of Speech (POS) tag for each word in a provided sentence.\n",
    "\n",
    "You are required to build a model using Hidden Markov Models which would help you predict the POS tags for all words in an utterance.\n",
    "\n",
    "### What is a POS tag?\n",
    "\n",
    "In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its contextâ€”i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset can be downloaded from https://drive.google.com/open?id=1345iaxqTImJN6mKGh_c1T5n2OWumpyYz. You can access it only using your BITS IDs.\n",
    "\n",
    "#### Dataset Description\n",
    "##### Sample Tuple\n",
    "b100-5507\n",
    "\n",
    "Mr.\tNOUN\n",
    "<br>\n",
    "Podger\tNOUN\n",
    "<br>\n",
    "had\tVERB\n",
    "<br>\n",
    "thanked\tVERB\n",
    "<br>\n",
    "him\tPRON\n",
    "<br>\n",
    "gravely\tADV\n",
    "<br>\n",
    ",\t.\n",
    "<br>\n",
    "and\tCONJ\n",
    "<br>\n",
    "now\tADV\n",
    "<br>\n",
    "he\tPRON\n",
    "<br>\n",
    "made\tVERB\n",
    "<br>\n",
    "use\tNOUN\n",
    "<br>\n",
    "of\tADP\n",
    "<br>\n",
    "the\tDET\n",
    "<br>\n",
    "advice\tNOUN\n",
    "<br>\n",
    ".\t.\n",
    "<br>\n",
    "##### Explanation\n",
    "The first token \"b100-5507\" is just a key and acts like an identifier to indicate the beginning of a sentence.\n",
    "<br>\n",
    "The other tokens have a (Word, POS Tag) pairing.\n",
    "\n",
    "__List of POS Tags are:__\n",
    ".\n",
    "<br>\n",
    "ADJ\n",
    "<br>\n",
    "ADP\n",
    "<br>\n",
    "ADV\n",
    "<br>\n",
    "CONJ\n",
    "<br>\n",
    "DET\n",
    "<br>\n",
    "NOUN\n",
    "<br>\n",
    "NUM\n",
    "<br>\n",
    "PRON\n",
    "<br>\n",
    "PRT\n",
    "<br>\n",
    "VERB\n",
    "<br>\n",
    "X\n",
    "\n",
    "__Note__\n",
    "<br>\n",
    "__.__ is used to indicate special characters such as '.', ','\n",
    "<br>\n",
    "__X__ is used to indicate vocab not part of Enlish Language mostly.\n",
    "Others are Standard POS tags.\n",
    "\n",
    "### Evaluation\n",
    "We wish to evaluate based on \n",
    "- coding practices being followed\n",
    "- commenting to explain the code and logic behind doing something\n",
    "- your understanding and explanation of data\n",
    "- how good the model would perform on unseen data.\n",
    "\n",
    "### Train-Test Split\n",
    "Let us use a 80-20 split of our data for training and evaluation purpose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict,OrderedDict,namedtuple\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read data\n",
    "Sentence = namedtuple(\"Sentence\", \"words tags\")\n",
    "\n",
    "\n",
    "def read_data_infile(filename):\n",
    "    \"\"\"Read tagged sentence data\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n",
    "    return OrderedDict(((s[0], Sentence(*zip(*[l.strip().split(\"\\t\")\n",
    "                        for l in s[1:]]))) for s in sentence_lines if s[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dict=read_data_infile('data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(words=('But', 'there', 'seemed', 'to', 'be', 'some', 'difference', 'of', 'opinion', 'as', 'to', 'how', 'far', 'the', 'board', 'should', 'go', ',', 'and', 'whose', 'advice', 'it', 'should', 'follow', '.'), tags=('CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADP', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB', 'VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['b100-935']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags=['ADJ' ,'ADP' ,'ADV' ,'CONJ' ,'DET' ,'NOUN' ,'NUM' ,'PRON' ,'PRT' ,'VERB' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pre-process data (Whatever you feel might be required)\n",
    "def read_required_tags(filename):\n",
    "    \"\"\"Read a list of word tag classes\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        tags = f.read().split(\"\\n\")\n",
    "    return frozenset(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_tags=read_required_tags('data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data Description\n",
    "keys = tuple(data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vocabulary of all unique words \n",
    "wordset = frozenset(chain(*[s.words for s in data_dict.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## word and tags \n",
    "sequences_word = tuple([data_dict[key].words for key in keys])\n",
    "sequences_tag = tuple([data_dict[key].tags for key in keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "dt = collections.defaultdict(list)\n",
    "def Subset(sentences,key):\n",
    "    return {x: sentences[x] for x in key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45872 11468\n"
     ]
    }
   ],
   "source": [
    "## train test split \n",
    "\n",
    "key_list = list(keys)\n",
    "random.shuffle(key_list)\n",
    "split = int( 0.8 * len(key_list))\n",
    "training_data = Subset(data_dict, key_list[:split])\n",
    "test_data = Subset(data_dict, key_list[split:])\n",
    "print(len(training_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 45872 sentences.\n",
      "Test set has 11468 sentences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set has {} sentences.\".format(len([data_dict[x] for x in training_data.keys()])))\n",
    "print(\"Test set has {} sentences.\\n\".format(len([data_dict[x] for x in  test_data.keys()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_allSets(my_dict,key_list):\n",
    "    word_sequences = tuple([my_dict[key].words for key in key_list])\n",
    "    tag_sequences = tuple([my_dict[key].tags for key in key_list])\n",
    "    wordset = frozenset(chain(*word_sequences))\n",
    "    tagset = frozenset(chain(*tag_sequences))\n",
    "    return (word_sequences,tag_sequences,wordset,tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_instances = get_allSets(training_data,training_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################ 2. COUNT NUMBER OF EACH TAG IN ENTIRE CORPUS #################\n",
    "########################  single_tag_counts[tag] = k  #########################\n",
    "all_sentences = [list(sentence) for sentence in [[data_dict[x] for x in training_data.keys()]]]\n",
    "all_tags = chain.from_iterable(all_sentences)\n",
    "single_tag_counts = dict(Counter(all_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## 3. COUNT NUMBER OF EACH TAG PAIR IN ENTIRE CORPUS ##############\n",
    "####################  pair_tag_counts[(tag_1, tag_2)] = k  ####################\n",
    "sentences = [s for s in all_sentences if len(s) > 1]  # discard any sequences of length 1\n",
    "pairs = []\n",
    "for s in sentences:\n",
    "    pairs.extend([(s[i-1], s[i]) for i in range(1, len(s))])\n",
    "\n",
    "pair_tag_counts = dict(Counter(pairs))\n",
    "\n",
    "if len(pair_tag_counts) < len(train_instances[3])**2:\n",
    "    for tag1 in train_instances[3]:\n",
    "        for tag2 in train_instances[3]:\n",
    "            if (tag1, tag2) not in pair_tag_counts:\n",
    "                pair_tag_counts[(tag1, tag2)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 4. COUNT NUMBER OF EACH TAG APPEARING IN THE BEGINNING or END OF SENTENCE ##\n",
    "#########################  start_tag_counts[tag] = k  #########################\n",
    "##########################  end_tag_counts[tag] = k  ##########################\n",
    "start_tag_counts = dict(Counter([sentence[0] for sentence in [[data_dict[x] for x in training_data.keys()]] ]))\n",
    "end_tag_counts = dict(Counter([sentence[-1] for sentence in [[data_dict[x] for x in training_data.keys()]]]))\n",
    "\n",
    "### if any tag has NO sentences starting/ending with it, set its value to 0:\n",
    "if len(start_tag_counts) < len(train_instances[3]):\n",
    "    for tag in train_instances[3]:\n",
    "        if tag not in start_tag_counts:\n",
    "            start_tag_counts[tag] = 0\n",
    "\n",
    "if len(end_tag_counts) < len(train_instances[3]):\n",
    "    for tag in train_instances[3]:\n",
    "        if tag not in end_tag_counts:\n",
    "            end_tag_counts[tag] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(words=('Mr.', 'Podger', 'had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.'), tags=('NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['b100-5507']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tag_sequences=train_instances[1]\n",
    "train_word_sequences=train_instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################## 5. COUNT NUMBER OF (TAG_i, WORD_i) PAIRS ###################\n",
    "#######################  pair_counts[tag][word] = k  ##########################\n",
    "pair_counts = defaultdict(lambda: defaultdict(lambda : 0))\n",
    "\n",
    "train_tag_sequences=train_instances[1]\n",
    "for sentence_idx, sentence in enumerate([training_data[x] for x in training_data.keys()]):\n",
    "    for word_idx, tag in enumerate(sentence):\n",
    "        word = train_tag_sequences[word_idx]\n",
    "        pair_counts[tag][word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HMM Model Goes Here\n",
    "HMM_model = HiddenMarkovModel(name = \"HMM-Tagger\")\n",
    "tag_states = [] \n",
    "for tag in train_instances[3]:\n",
    "    tag_emissions = DiscreteDistribution({word:pair_counts[tag][word]/single_tag_counts[tag] \\\n",
    "                                          for word in train_instances[2]})\n",
    "    tag_state = State(tag_emissions, name = tag)\n",
    "    tag_states.append(tag_state)\n",
    "    HMM_model.add_states(tag_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model Accuracy Evaluation\n",
    "n_sentences = len(data.training_set.keys)\n",
    "\n",
    "for tag_state1 in tag_states:\n",
    "    for tag_state2 in tag_states:\n",
    "        tag1, tag2 = tag_state1.name, tag_state2.name\n",
    "        HMM_model.add_transition(HMM_model.start, tag_state1, start_tag_counts[tag1]/n_sentences)\n",
    "        HMM_model.add_transition(tag_state1, HMM_model.end, end_tag_counts[tag1]/single_tag_counts[tag1])\n",
    "        HMM_model.add_transition(tag_state1, tag_state2, pair_tag_counts[(tag1,tag2)]/single_tag_counts[tag1])\n",
    "    \n",
    "HMM_model.bake()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 78.38%\n"
     ]
    }
   ],
   "source": [
    "#Adds code blocks wherever you feel necessary\n",
    "\n",
    "####################### 8. MAKE PREDICTIONS ON TEST SET #######################\n",
    "test_correct = 0 \n",
    "test_count = 0   \n",
    "\n",
    "### ITERATE PER SENTENCE\n",
    "for words, true_tags in zip(train_tag_sequences, train_word_sequences):\n",
    "    try: \n",
    "        # Only consider words contained in training set's vocab\n",
    "        _, viterbi_path = HMM_model.viterbi([w if w in train_instances[2] else 'nan' for w in words])\n",
    "        predicted_tags = [state[1].name for state in viterbi_path[1:-1]] \n",
    "        test_correct += sum(pred == true for pred, true in zip(predicted_tags, true_tags))\n",
    "        \n",
    "        if print_i == 101: # print a sample result\n",
    "            print(\"Test Sentence: \\n\", words)\n",
    "            print()\n",
    "            print(\"Predicted Tags: \\n\", predicted_tags)\n",
    "            print()\n",
    "            print(\"True Tags: \\n\", true_tags)\n",
    "            print_i += 1\n",
    "    except:\n",
    "        pass\n",
    "    test_count += len(words)\n",
    "\n",
    "test_acc = test_correct/test_count\n",
    "print(\"\\nTest Accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy Coding!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
