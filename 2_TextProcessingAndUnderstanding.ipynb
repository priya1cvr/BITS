{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_TextProcessingAndUnderstanding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5OIKc1cl6flOptp4fyIsR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTzayjTUjaZi",
        "colab_type": "text"
      },
      "source": [
        "## Text Preprocessing and Wrangling\n",
        "\n",
        "- Text wrangling (also called preprocessing or normalization) is a process that consists of\n",
        "a series of steps to wrangle, clean, and standardize textual data into a form that could be consumed by other NLP and intelligent systems powered by machine learning and deep learning. \n",
        "\n",
        "- Common techniques for preprocessing include cleaning text, tokenizing text, removing special characters, case conversion, correcting spellings, removing stopwords and other unnecessary terms, stemming, and lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f46izDLkCyT",
        "colab_type": "text"
      },
      "source": [
        "## Removing HTML Tags\n",
        "\n",
        "-  Unstructured text contains a lot of noise, especially if you use techniques\n",
        "like web scraping or screen scraping to retrieve data from web pages, blogs, and online repositories.\n",
        "\n",
        "- HTML tags, JavaScript, and Iframe tags typically don’t add much value to understanding and analyzing text.\n",
        "\n",
        "- Our main intent is to extract meaningful textual content from the data extracted from the web"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-YvOrlRkPeQ",
        "colab_type": "code",
        "outputId": "f4a0181e-c833-49c3-ff39-b726371f1bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "## web scraping - retrieve the contents of this web page in Python\n",
        "import requests \n",
        "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
        "content = data.content\n",
        "print(content[1163:2200])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'a name=\"generator\" content=\"Ebookmaker 0.8.9 by Project Gutenberg\"/>\\r\\n</head>\\r\\n  <body><p id=\"id00000\">Project Gutenberg EBook The Bible, King James, Book 1: Genesis</p>\\r\\n\\r\\n<p id=\"id00001\">Copyright laws are changing all over the world. Be sure to check the\\r\\ncopyright laws for your country before downloading or redistributing\\r\\nthis or any other Project Gutenberg eBook.</p>\\r\\n\\r\\n<p id=\"id00002\">This header should be the first thing seen when viewing this Project\\r\\nGutenberg file.  Please do not remove it.  Do not change or edit the\\r\\nheader without written permission.</p>\\r\\n\\r\\n<p id=\"id00003\">Please read the \"legal small print,\" and other information about the\\r\\neBook and Project Gutenberg at the bottom of this file.  Included is\\r\\nimportant information about your specific rights and restrictions in\\r\\nhow the file may be used.  You can also find out about how to make a\\r\\ndonation to Project Gutenberg, and how to get involved.</p>\\r\\n\\r\\n<p id=\"id00004\" style=\"margin-top: 2em\">**Welcome To The World of Free Plain Vanilla Electronic Texts'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5zBKqD6k7WX",
        "colab_type": "text"
      },
      "source": [
        "- We can clearly see from the preceding output that it is extremely difficult to decipher the actual textual content in the web page, due to all the unnecessary HTML tags. We need to remove those tags. \n",
        "- The BeautifulSoup library provides us with some handy functions that help us remove these unnecessary tags with ease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb4cQZiSlCSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re \n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text,\"html.parser\")\n",
        "  [s.extract() for s in soup(['iframe','script'])]\n",
        "  stripped_text=soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+','\\n',stripped_text)\n",
        "  return stripped_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM4wKOn7mTvi",
        "colab_type": "code",
        "outputId": "9e0f88e4-4519-4984-d354-fa02c942c0b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "## Call the function above\n",
        "clean_content = strip_html_tags(content)\n",
        "print(clean_content[1163:2045])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***\n",
            "This eBook was produced by David Widger\n",
            "with the help of Derek Andrew's text from January 1992\n",
            "and the work of Bryan Taylor in November 2002.\n",
            "Book 01        Genesis\n",
            "01:001:001 In the beginning God created the heaven and the earth.\n",
            "01:001:002 And the earth was without form, and void; and darkness was\n",
            "           upon the face of the deep. And the Spirit of God moved upon\n",
            "           the face of the waters.\n",
            "01:001:003 And God said, Let there be light: and there was light.\n",
            "01:001:004 And God saw the light, that it was good: and God divided the\n",
            "           light from the darkness.\n",
            "01:001:005 And God called the light Day, and the darkness he called\n",
            "           Night. And the evening and the morning were the first day.\n",
            "01:001:006 And God said, Let there be a firmament in the midst of the\n",
            "           waters,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9nNQ2FZmq7L",
        "colab_type": "text"
      },
      "source": [
        "- We have successfully removed the unnecessary HTML tags. We now have a clean body of text that’s easier to interpret and understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcXRFhnWsv5R",
        "colab_type": "text"
      },
      "source": [
        "#### Text Tokenization\n",
        "- defined as the process of breaking down or splitting textual data into smaller and more meaningful components called tokens\n",
        "\n",
        "#### Sentence Tokenization\n",
        "- process of splitting a text corpus into sentences that act\n",
        "as the first level of tokens the corpus is comprised of. This is also known as sentence segmentation, since we try to segment the text into meaningful sentences. \n",
        "- Below are main  sentence tokenizers:\n",
        " -  sent_tokenize\n",
        " -  Pretrained sentence tokenization models \n",
        " -  PunktSentenceTokenizer\n",
        " - RegexpTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKV9p7_xmhfT",
        "colab_type": "code",
        "outputId": "af817683-99c0-4952-fc37-d150041f4a4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## Use tokenize on some sample text and part of the Gutenberg corpus available in NLTK\n",
        "\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK8nXjG3vD7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## loading text corpora \n",
        "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
        "sample_text = (\"US unveils world's most powerful supercomputer, beats China.\" \n",
        "               \"The US has unveiled the world's most powerful supercomputer called 'Summit',\" \n",
        "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
        "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,\" \n",
        "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
        "               \"which reportedly take up the size of two tennis courts.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l2wNHZWwUJb",
        "colab_type": "code",
        "outputId": "1a6340cf-1534-4d45-e97d-d12ef56b3b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sample_text"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China.The US has unveiled the world's most powerful supercomputer called 'Summit',beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtB-NV8_wXJ4",
        "colab_type": "code",
        "outputId": "9dbb9aeb-a8e4-42aa-b8e7-b86682de2e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Total characters in Alice in Wonderland\n",
        "print(len(alice))\n",
        "\n",
        "# First 100 characters in the corpus\n",
        "alice[0:100]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "144395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsP5lpTKgGpB",
        "colab_type": "text"
      },
      "source": [
        "### Default Sentence Tokenizer \n",
        "- The nltk.sent_tokenize() function is the default sentence tokenization function that NLTK recommends and it uses an instance of the PunktSentenceTokenizer class internally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ_dcNshg8tB",
        "colab_type": "code",
        "outputId": "a679d521-d0a1-4e7a-db62-f8ef00d3ff8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E05g_Y-Ywhtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "default_st = nltk.sent_tokenize\n",
        "alice_sentences = default_st(text=alice)\n",
        "sample_sentences = default_st(text=sample_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTWYrguRg3eZ",
        "colab_type": "code",
        "outputId": "7377d463-6acf-4afd-cffc-88dd7a705e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "print('Total sentences in sample_text:', len(sample_sentences))\n",
        "print('Sample text sentences :-')\n",
        "print(np.array(sample_sentences))\n",
        "print(\"------------------------\")\n",
        "print('\\nTotal sentences in alice:', len(alice_sentences))\n",
        "print('First 5 sentences in alice:-')\n",
        "print(np.array(alice_sentences[0:5]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total sentences in sample_text: 3\n",
            "Sample text sentences :-\n",
            "[\"US unveils world's most powerful supercomputer, beats China.The US has unveiled the world's most powerful supercomputer called 'Summit',beating the previous record-holder China's Sunway TaihuLight.\"\n",
            " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,which is capable of 93,000 trillion calculations per second.'\n",
            " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n",
            "------------------------\n",
            "\n",
            "Total sentences in alice: 1625\n",
            "First 5 sentences in alice:-\n",
            "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\"\n",
            " \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\"\n",
            " 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.'\n",
            " \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\"\n",
            " 'Oh dear!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB-kNRIfhjJ0",
        "colab_type": "text"
      },
      "source": [
        "-  Note that: This  doesn’t just use periods to delimit sentences, but also considers other punctuation and capitalization of words. We can also tokenize text of other languages using some pretrained models present in NLTK.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHhjnR2uh8l-",
        "colab_type": "text"
      },
      "source": [
        "### Pretrained Sentence Tokenizer Models\n",
        "- Suppose we were dealing with German text. We can use sent_tokenize, which\n",
        "is already trained, or load a pretrained tokenization model on German text into a PunktSentenceTokenizer instance and perform the same operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lEVUTFWiiIx",
        "colab_type": "code",
        "outputId": "75b7ffb6-014d-441a-f373-d4f79f019ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('europarl_raw')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package europarl_raw to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/europarl_raw.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeMVMjWShCop",
        "colab_type": "code",
        "outputId": "86c7b1ec-d3b7-495a-e1de-7240275c0021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from nltk.corpus import europarl_raw\n",
        "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
        "# Total characters in the corpus\n",
        "print(len(german_text))\n",
        "# First 100 characters in the corpus\n",
        "print(german_text[0:100])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157171\n",
            " \n",
            "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1i1y6yui_Dt",
        "colab_type": "text"
      },
      "source": [
        "- Next, we tokenize the text corpus into sentences using the default sent_ tokenize(...) tokenizer and a pretrained German language tokenizer by loading it from the NLTK resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUgz8K3CifiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# default sentence tokenizer\n",
        "german_sentences_def = default_st(german_text,language='german')\n",
        "\n",
        "# loading german text tokenizer into a PunktSentenceTokenizer instance\n",
        "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
        "german_sentences = german_tokenizer.tokenize(german_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkFR24Dqj6-O",
        "colab_type": "code",
        "outputId": "4cf270d7-24be-45f8-acc8-fbaf4ea73ce5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## check if the results obtained by using the two tokenizers match!\n",
        "\n",
        "# verify the type of german_tokenizer\n",
        "# should be PunktSentenceTokenizer\n",
        "print(type(german_tokenizer))\n",
        "\n",
        "# check if results of both tokenizers match\n",
        "# should be True\n",
        "print(german_sentences_def == german_sentences)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUIVFzuhkh5E",
        "colab_type": "text"
      },
      "source": [
        "- Obs: german_tokenizer is an instance of PunktSentenceTokenizer, which specializes in dealing with the German language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2blHPYOkxYz",
        "colab_type": "text"
      },
      "source": [
        "#### PunktSentenceTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0luqymVIkLZQ",
        "colab_type": "code",
        "outputId": "3d03d963-fcb7-4d9b-b327-712cb75b2368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "punkt_st= nltk.tokenize.PunktSentenceTokenizer()\n",
        "sample_sentences = punkt_st.tokenize(sample_text)\n",
        "for sent in sample_sentences:\n",
        "  print(sent,\"\\n\")\n",
        "print(len(sample_sentences))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "US unveils world's most powerful supercomputer, beats China.The US has unveiled the world's most powerful supercomputer called 'Summit',beating the previous record-holder China's Sunway TaihuLight. \n",
            "\n",
            "With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,which is capable of 93,000 trillion calculations per second. \n",
            "\n",
            "Summit has 4,608 servers, which reportedly take up the size of two tennis courts. \n",
            "\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c99aVunQlYzo",
        "colab_type": "text"
      },
      "source": [
        "### RegexpTokenizer\n",
        "\n",
        "- We will use specific regular expression-based patterns to segment sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQcg21gVlHr6",
        "colab_type": "code",
        "outputId": "095ada73-fede-48e7-aa38-b2db1cca777e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
        "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN,gaps=True)\n",
        "sample_sentences = regex_st.tokenize(sample_text)\n",
        "for sent in sample_sentences:\n",
        "  print(sent,\"\\n\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "US unveils world's most powerful supercomputer, beats China.The US has unveiled the world's most powerful supercomputer called 'Summit',beating the previous record-holder China's Sunway TaihuLight. \n",
            "\n",
            "With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,which is capable of 93,000 trillion calculations per second. \n",
            "\n",
            "Summit has 4,608 servers, which reportedly take up the size of two tennis courts. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkXDrxw8ncJe",
        "colab_type": "text"
      },
      "source": [
        "<b> In the following section, we look at tokenizing these sentences into words using several techniques. </b>\n",
        "\n",
        "### Word Tokenization\n",
        "\n",
        "- It is the process of splitting or segmenting sentences into their constituent words. \n",
        "- A sentence is a collection of words and with tokenization we essentially split a sentence into a list of words that can be used to reconstruct the sentence. \n",
        "- Word tokenization is really important in many processes, especially in cleaning and normalizing text where operations like stemming and lemmatization work on each individual word based on its respective stems and lemma. \n",
        "- NLTK provides various useful interfaces for word tokenization. We will touch up on the following main interfaces:\n",
        "  -  word_tokenize\n",
        "  -  TreebankWordTokenizer\n",
        "  -  TokTokTokenizer\n",
        "  -  RegexpTokenizer\n",
        "  -  Inherited tokenizers from RegexpTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnkkSeNgpL8U",
        "colab_type": "text"
      },
      "source": [
        "### Default Word Tokenizer\n",
        "- The nltk.word_tokenize(...) function is the default and recommended word tokenizer, as specified by NLTK. \n",
        "- This tokenizer is an instance or object of the TreebankWordTokenizer class in its internal implementation and acts as a wrapper to that core class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0szHm5L_mIM3",
        "colab_type": "code",
        "outputId": "498dc9af-83c0-4a09-ceb3-553899ef486c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "default_wt = nltk.word_tokenize\n",
        "words = default_wt(sample_text)\n",
        "print(len(words))\n",
        "np.array(words)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China.The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
              "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGsUM7t7p0yU",
        "colab_type": "text"
      },
      "source": [
        "### TreebankWordTokenizer\n",
        "- The TreebankWordTokenizer is based on the Penn Treebank and uses various regular expressions to tokenize the text.\n",
        "- One <b>primary assumption</b> here is that we have already performed sentence tokenization beforehand\n",
        "- Some of the main features of this tokenizer are:\n",
        "     - Splits and separates out periods that appear at the end of a sentence\n",
        "     - Splits and separates commas and single quotes when followed by whitespace\n",
        "     - Most punctuation characters are split and separated into independent tokens\n",
        "     - Splits words with standard contractions, such as don’t to do and n’t\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeN6yayNppMp",
        "colab_type": "code",
        "outputId": "fe3435c7-d308-4651-f0a8-0eee852e34bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "treebank_wt = nltk.TreebankWordTokenizer()\n",
        "words = treebank_wt.tokenize(sample_text)\n",
        "print(len(words))\n",
        "np.array(words)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "77\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China.The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway',\n",
              "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrsolXTGtMOk",
        "colab_type": "text"
      },
      "source": [
        "### TokTokTokenizer\n",
        "- TokTokTokenizer is one of the newer tokenizers introduced by NLTK present in the nltk.tokenize.toktok module.\n",
        "- In general, the tok-tok tokenizer is a general tokenizer, where it assumes that the input has one sentence per line. Hence, only the final period is tokenized. \n",
        "- However, as needed, we can remove the other periods from the words using regular expressions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqZ06WUwsxYZ",
        "colab_type": "code",
        "outputId": "963f60bb-e1c9-41c5-e3d2-13a3be4a4f66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer = ToktokTokenizer()\n",
        "words = tokenizer.tokenize(sample_text)\n",
        "print(len(words))\n",
        "np.array(words)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "81\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China.The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating',\n",
              "       'the', 'previous', 'record-holder', 'China', \"'\", 's', 'Sunway',\n",
              "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUQU_gibuP6d",
        "colab_type": "text"
      },
      "source": [
        "### RegexpTokenizer\n",
        "- There are two main parameters that\n",
        "are useful in tokenization—the regex pattern for building the tokenizer and the gaps parameter, which, if set to true, is used to find the gaps between the tokens. Otherwise, it is used to find the tokens themselves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bdz_d0ss-n2",
        "colab_type": "code",
        "outputId": "42fc5561-220c-4f78-d550-96df042ecf42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# pattern to identify tokens themselves\n",
        "TOKEN_PATTERN = r'\\W+'\n",
        "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN,gaps=True)\n",
        "words = regex_wt.tokenize(sample_text)\n",
        "print(len(words))\n",
        "np.array(words)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "75\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', 's', 'most', 'powerful', 'supercomputer',\n",
              "       'beats', 'China', 'The', 'US', 'has', 'unveiled', 'the', 'world',\n",
              "       's', 'most', 'powerful', 'supercomputer', 'called', 'Summit',\n",
              "       'beating', 'the', 'previous', 'record', 'holder', 'China', 's',\n",
              "       'Sunway', 'TaihuLight', 'With', 'a', 'peak', 'performance', 'of',\n",
              "       '200', '000', 'trillion', 'calculations', 'per', 'second', 'it',\n",
              "       'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight',\n",
              "       'which', 'is', 'capable', 'of', '93', '000', 'trillion',\n",
              "       'calculations', 'per', 'second', 'Summit', 'has', '4', '608',\n",
              "       'servers', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P8HgtIxvGTF",
        "colab_type": "code",
        "outputId": "064d0d9a-9e80-4dd7-c870-f28888502f14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# pattern to identify tokens by using gaps between tokens\n",
        "GAP_PATTERN = r'\\s+'\n",
        "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN,gaps=True)\n",
        "words = regex_wt.tokenize(sample_text)\n",
        "print(len(words))\n",
        "np.array(words)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', \"world's\", 'most', 'powerful', 'supercomputer,',\n",
              "       'beats', 'China.The', 'US', 'has', 'unveiled', 'the', \"world's\",\n",
              "       'most', 'powerful', 'supercomputer', 'called', \"'Summit',beating\",\n",
              "       'the', 'previous', 'record-holder', \"China's\", 'Sunway',\n",
              "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second,', 'it', 'is', 'over',\n",
              "       'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight,which', 'is',\n",
              "       'capable', 'of', '93,000', 'trillion', 'calculations', 'per',\n",
              "       'second.', 'Summit', 'has', '4,608', 'servers,', 'which',\n",
              "       'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis',\n",
              "       'courts.'], dtype='<U16')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bemxj3EB0N6K",
        "colab_type": "text"
      },
      "source": [
        "### Inherited Tokenizers from RegexpTokenizer\n",
        "- Besides the base RegexpTokenizer class, there are several derived classes that\n",
        "perform different types of word tokenization. \n",
        "- The WordPunktTokenizer uses the pattern r'\\w+|[^\\w\\s]+' to tokenize sentences into independent alphabetic and non-alphabetic tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cddH8K4wVhD",
        "colab_type": "code",
        "outputId": "bfcd75ef-b5f2-4c9f-d9ae-dfd1767fcf2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "wordpunkt_wt =nltk.WordPunctTokenizer()\n",
        "words = wordpunkt_wt.tokenize(sample_text)\n",
        "print(len(words))\n",
        "np.array(words)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'\", 'Summit', \"',\", 'beating', 'the',\n",
              "       'previous', 'record', '-', 'holder', 'China', \"'\", 's', 'Sunway',\n",
              "       'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200',\n",
              "       ',', '000', 'trillion', 'calculations', 'per', 'second', ',', 'it',\n",
              "       'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight',\n",
              "       ',', 'which', 'is', 'capable', 'of', '93', ',', '000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4', ',',\n",
              "       '608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the',\n",
              "       'size', 'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL8UFj2Q1pci",
        "colab_type": "code",
        "outputId": "d5d42378-caef-426e-ebbf-47c2bf3cab65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "### The WhitespaceTokenizer tokenizes sentences into words based on whitespace, like tabs, newlines, and spaces\n",
        "whitespace_wt = nltk.WhitespaceTokenizer()\n",
        "words = whitespace_wt.tokenize(sample_text)\n",
        "print(len(words))\n",
        "np.array(words)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', \"world's\", 'most', 'powerful', 'supercomputer,',\n",
              "       'beats', 'China.The', 'US', 'has', 'unveiled', 'the', \"world's\",\n",
              "       'most', 'powerful', 'supercomputer', 'called', \"'Summit',beating\",\n",
              "       'the', 'previous', 'record-holder', \"China's\", 'Sunway',\n",
              "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second,', 'it', 'is', 'over',\n",
              "       'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight,which', 'is',\n",
              "       'capable', 'of', '93,000', 'trillion', 'calculations', 'per',\n",
              "       'second.', 'Summit', 'has', '4,608', 'servers,', 'which',\n",
              "       'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis',\n",
              "       'courts.'], dtype='<U16')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6Fwkl552Jrh",
        "colab_type": "code",
        "outputId": "77f84e4c-7111-4c3a-d63a-d529e4bac3fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "#obtain the token boundaries for each token during the tokenize operation\n",
        "word_indices = list(regex_wt.span_tokenize(sample_text))\n",
        "print(word_indices)\n",
        "print(np.array([sample_text[start:end] for start,end in word_indices]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 2), (3, 10), (11, 18), (19, 23), (24, 32), (33, 47), (48, 53), (54, 63), (64, 66), (67, 70), (71, 79), (80, 83), (84, 91), (92, 96), (97, 105), (106, 119), (120, 126), (127, 143), (144, 147), (148, 156), (157, 170), (171, 178), (179, 185), (186, 197), (198, 202), (203, 204), (205, 209), (210, 221), (222, 224), (225, 232), (233, 241), (242, 254), (255, 258), (259, 266), (267, 269), (270, 272), (273, 277), (278, 283), (284, 286), (287, 291), (292, 294), (295, 301), (302, 318), (319, 321), (322, 329), (330, 332), (333, 339), (340, 348), (349, 361), (362, 365), (366, 373), (374, 380), (381, 384), (385, 390), (391, 399), (400, 405), (406, 416), (417, 421), (422, 424), (425, 428), (429, 433), (434, 436), (437, 440), (441, 447), (448, 455)]\n",
            "['US' 'unveils' \"world's\" 'most' 'powerful' 'supercomputer,' 'beats'\n",
            " 'China.The' 'US' 'has' 'unveiled' 'the' \"world's\" 'most' 'powerful'\n",
            " 'supercomputer' 'called' \"'Summit',beating\" 'the' 'previous'\n",
            " 'record-holder' \"China's\" 'Sunway' 'TaihuLight.' 'With' 'a' 'peak'\n",
            " 'performance' 'of' '200,000' 'trillion' 'calculations' 'per' 'second,'\n",
            " 'it' 'is' 'over' 'twice' 'as' 'fast' 'as' 'Sunway' 'TaihuLight,which'\n",
            " 'is' 'capable' 'of' '93,000' 'trillion' 'calculations' 'per' 'second.'\n",
            " 'Summit' 'has' '4,608' 'servers,' 'which' 'reportedly' 'take' 'up' 'the'\n",
            " 'size' 'of' 'two' 'tennis' 'courts.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UDJwfW3om82",
        "colab_type": "text"
      },
      "source": [
        "### Building Robust Tokenizers with NLTK and spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovEIfauhoARn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_text(text):\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyJ9O7jqpy5q",
        "colab_type": "code",
        "outputId": "471be728-9ed6-452f-bf4a-6b40da3aae83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "sents = tokenize_text(sample_text)\n",
        "np.array(sents)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China.The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.']),\n",
              "       list(['With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.']),\n",
              "       list(['Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiA0A7bCp2eD",
        "colab_type": "code",
        "outputId": "2b75a7c3-2ed4-4c9e-98fa-a5a45c2c49aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "words = [word for sentence in sents for word in sentence]\n",
        "print(len(words))\n",
        "np.array(words)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China.The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
              "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy08NReWqQUv",
        "colab_type": "code",
        "outputId": "b1359721-03d0-4f62-e53f-ad974a2be7af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "## In a similar way, we can leverage spaCy to perform sentence- and word-level tokenizations really quickly\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm',parse=True,tag=True,entity=True)\n",
        "text_spacy =nlp(sample_text)\n",
        "\n",
        "sents =np.array(list(text_spacy.sents))\n",
        "print(len(sents))\n",
        "sents"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([US unveils world's most powerful supercomputer, beats China.,\n",
              "       The US has unveiled the world's most powerful supercomputer called 'Summit',beating the previous record-holder,\n",
              "       China's Sunway TaihuLight.,\n",
              "       With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,which is capable of 93,000 trillion calculations per second.,\n",
              "       Summit has 4,608 servers, which reportedly take up the size of two tennis courts.],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_ZdKQsIq0pl",
        "colab_type": "code",
        "outputId": "0f6bf18e-537f-47d4-886f-c82e4b0f609e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "sent_words = [[word.text for word in sent] for sent in sents]\n",
        "np.array(sent_words)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.']),\n",
              "       list(['The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'\", \"Summit',beating\", 'the', 'previous', 'record', '-', 'holder']),\n",
              "       list(['China', \"'s\", 'Sunway', 'TaihuLight', '.']),\n",
              "       list(['With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.']),\n",
              "       list(['Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67z_Eo0jrbk3",
        "colab_type": "code",
        "outputId": "ecd603f8-c530-4fdb-a6f5-fb66c63d4762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "words = [word.text for word in text_spacy]\n",
        "print(len(words))\n",
        "np.array(words)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "81\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'\", \"Summit',beating\", 'the',\n",
              "       'previous', 'record', '-', 'holder', 'China', \"'s\", 'Sunway',\n",
              "       'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of',\n",
              "       '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it',\n",
              "       'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight',\n",
              "       ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U15')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzBsTRCBsLhZ",
        "colab_type": "text"
      },
      "source": [
        "## Removing Accented Characters\n",
        "\n",
        "- Usually in any text corpus, you might be dealing with accented characters/letters, especially if you only want to analyze the English language. Hence, we need to make sure that these characters are converted and standardized into ASCII characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk-FuPoxr3ZI",
        "colab_type": "code",
        "outputId": "72be607b-2f37-4efa-f979-94f9aa39061a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import unicodedata\n",
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')\n",
        "  return text \n",
        "\n",
        "remove_accented_chars('Sómě Áccěntěd těxt')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some Accented text'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwfW2wVxy9sX",
        "colab_type": "text"
      },
      "source": [
        "### Stemming\n",
        "- Morphemes are the smallest independent unit in any natural language\n",
        "- Morphemes consist of units that are stems and affixes.\n",
        "- Affixes are units like prefixes, suffixes, and so on, which are attached to word stems to change their meaning or create a new word altogether.\n",
        "- Word stems are also often known as the base form of a word and we can create new words by attaching affixes to them. This process is known as inflection\n",
        "- The reverse of this is obtaining the base form of a word from its inflected form and this is known as stemming\n",
        "- Consider the word “JUMP”, you can add affixes to it and form several new words like “JUMPS”, “JUMPED”, and “JUMPING”. In this case, the base word is “JUMP” and this is the word stem\n",
        "- In this case, the base word is “JUMP” and this is the word stem. If we were to carry out stemming on any of its three inflected forms, we would get the base form. \n",
        "- Stemming helps us standardize words to their base stem irrespective of their inflections, which helps many applications like classifying or clustering text or even in information retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7ujq3tjyQDS",
        "colab_type": "code",
        "outputId": "f2b6882a-f06c-4cab-e64f-9881d54cec76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Porter Stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "ps.stem('jumping'),ps.stem('jumps'),ps.stem('jumped'),ps.stem('lying'),ps.stem('strange')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump', 'lie', 'strang')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBxittdRE2uo",
        "colab_type": "code",
        "outputId": "1534b099-0c7b-4178-b578-272a0edc643f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Lancaster Stemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "ls =LancasterStemmer()\n",
        "ls.stem('jumping'),ls.stem('jumped'),ls.stem('jumped'),ls.stem('lying'),ls.stem('are'),ls.stem('strange')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump', 'lying', 'ar', 'strange')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq7UwRARFhj9",
        "colab_type": "text"
      },
      "source": [
        "- Observation: \n",
        "You can see the behavior of this stemmer is different from the previous Porter stemmer. \n",
        "Besides these two, there are several other stemmers, including RegexpStemmer, where you can build your own stemmer \n",
        "based on user-defined rules and SnowballStemmer, which supports stemming in 13 different languages besides English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6s0boLNFQq_",
        "colab_type": "code",
        "outputId": "bdf45026-c171-49b5-94dc-966eff47654b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
        "rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped'),rs.stem('lying'),rs.stem('strange')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump', 'ly', 'strange')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zALZA9AuFdar",
        "colab_type": "code",
        "outputId": "a1b68318-6634-43c4-cc8d-f36346b8d0be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Snowball Stemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "ss = SnowballStemmer(\"german\")\n",
        "print('Supported Languages:', SnowballStemmer.languages)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Supported Languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBH7y3zXFzU1",
        "colab_type": "code",
        "outputId": "6534532f-9902-4ef0-e6a4-bf53a2e8d149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# stemming on German words\n",
        "# autobahnen -> cars\n",
        "# autobahn -> car\n",
        "ss.stem('autobahnen'),ss.stem('springen')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('autobahn', 'spring')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlSdgwWzL_6C",
        "colab_type": "text"
      },
      "source": [
        "### Lemmatization\n",
        "- The process of lemmatization is very similar to stemming, where we remove word affixes to get to a base form of the word\n",
        "- However in this case, this base form is also known\n",
        "as the root word but not the root stem\n",
        "- The <b> difference between the two </b> is that the root stem may not always be a lexicographically correct word, i.e., it may not be present in the dictionary but the root word, also known as the lemma, will always be present in the dictionary.\n",
        "- The lemmatization process is considerably slower than stemming because an additional step is involved where the root form or lemma is formed by removing the affix from the word if and only if the lemma is present in the dictionary.\n",
        "- The NLTK package has a robust lemmatization module where it uses WordNet and the word’s syntax and semantics like part of speech and context to get the root word or lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu5PwlYJOJjH",
        "colab_type": "code",
        "outputId": "a3cf7ce9-0c46-4b9e-d550-10f3a872b9dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjT4YRAjF7Vp",
        "colab_type": "code",
        "outputId": "00bbdd3d-671c-466a-ccbe-88ce977daa59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "# lemmatize nouns\n",
        "print(wnl.lemmatize('cars','n'))\n",
        "print(wnl.lemmatize('men', 'n'))\n",
        "print(wnl.lemmatize('lying', 'n'))\n",
        "print(wnl.lemmatize('jumped', 'n'))\n",
        "print(wnl.lemmatize('jumps', 'n'))\n",
        "# lemmatize verbs\n",
        "print(wnl.lemmatize('jumps', 'v'))\n",
        "print(wnl.lemmatize('jumping','n'))\n",
        "print(wnl.lemmatize('running', 'v'))\n",
        "print(wnl.lemmatize('ate', 'v'))\n",
        "\n",
        "# lemmatize adjectives\n",
        "print(wnl.lemmatize('saddest', 'a'))\n",
        "print(wnl.lemmatize('fancier', 'a'))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car\n",
            "men\n",
            "lying\n",
            "jumped\n",
            "jump\n",
            "jump\n",
            "jumping\n",
            "run\n",
            "eat\n",
            "sad\n",
            "fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fcV8q3IO08L",
        "colab_type": "text"
      },
      "source": [
        "- This snippet shows us how each word is converted to its base form using lemmatization. \n",
        "- This helps us standardize words. \n",
        "- This code leverages the WordNetLemmatizer class, which internally uses the morphy() function belonging to the WordNetCorpusReader class. \n",
        "- This function basically finds the base form or lemma for a given word using the word and its part of speech by checking the WordNet corpus and uses a recursive technique for removing affixes from the word until a match is found\n",
        "in WordNet. \n",
        "  - If no match is found, the input word is returned unchanged. The part of speech is extremely important because if that is wrong, the lemmatization will not be effective, as you can see in the following snippet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7WhgquBOG8E",
        "colab_type": "code",
        "outputId": "93a591d1-0b12-4d73-eb02-c36eea0a5783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# ineffective lemmatization\n",
        "print (wnl.lemmatize('ate', 'n'))\n",
        "print (wnl.lemmatize('fancier', 'v'))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ate\n",
            "fancier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow1GtH38CREx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}